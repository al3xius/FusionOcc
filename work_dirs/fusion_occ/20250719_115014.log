2025-07-19 11:50:14,237 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3070
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.12.0
MMCV: 1.6.0
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+f8f917b
spconv2.0: False
------------------------------------------------------------

2025-07-19 11:50:14,778 - mmdet - INFO - Distributed training: False
2025-07-19 11:50:15,265 - mmdet - INFO - Config:
point_cloud_range = [-40, -40, -1, 40, 40, 5.4]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDatasetOccpancy'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='PrepareImageSeg',
        downsample=1,
        is_train=True,
        data_config=dict(
            cams=[
                'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
            ],
            Ncams=6,
            input_size=(512, 1408),
            src_size=(900, 1600),
            resize=(-0.06, 0.11),
            rot=(-5.4, 5.4),
            flip=True,
            crop_h=(0.0, 0.0),
            resize_test=0.0),
        sequential=True,
        img_seg_dir='data/nuscenes/imgseg/samples'),
    dict(type='LoadOccGTFromFile'),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='FuseAdjacentSweeps',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(type='PointsLidar2Ego'),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
    dict(
        type='LoadAnnotationsAll',
        bda_aug_conf=dict(
            rot_lim=(-0.0, 0.0),
            scale_lim=(1.0, 1.0),
            flip_dx_ratio=0.5,
            flip_dy_ratio=0.5),
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        is_train=True),
    dict(
        type='PointToMultiViewDepth',
        downsample=1,
        grid_config=dict(
            x=[-40, 40, 0.4],
            y=[-40, 40, 0.4],
            z=[-1, 5.4, 0.4],
            depth=[1.0, 45.0, 0.5])),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=[
            'img_inputs', 'points', 'sparse_depth', 'segs', 'voxel_semantics',
            'mask_camera'
        ])
]
test_pipeline = [
    dict(
        type='PrepareImageSeg',
        restore_upsample=8,
        downsample=1,
        data_config=dict(
            cams=[
                'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
            ],
            Ncams=6,
            input_size=(512, 1408),
            src_size=(900, 1600),
            resize=(-0.06, 0.11),
            rot=(-5.4, 5.4),
            flip=True,
            crop_h=(0.0, 0.0),
            resize_test=0.0),
        sequential=True,
        img_seg_dir='data/nuscenes/imgseg/samples'),
    dict(type='LoadOccGTFromFile'),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='FuseAdjacentSweeps',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(type='PointsLidar2Ego'),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
    dict(
        type='LoadAnnotationsAll',
        bda_aug_conf=dict(
            rot_lim=(-0.0, 0.0),
            scale_lim=(1.0, 1.0),
            flip_dx_ratio=0.5,
            flip_dy_ratio=0.5),
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        is_train=False),
    dict(
        type='PointToMultiViewDepth',
        downsample=1,
        grid_config=dict(
            x=[-40, 40, 0.4],
            y=[-40, 40, 0.4],
            z=[-1, 5.4, 0.4],
            depth=[1.0, 45.0, 0.5])),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='Collect3D',
                keys=['img_inputs', 'points', 'sparse_depth'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/fusionocc-nuscenes_infos_train.pkl',
        pipeline=[
            dict(
                type='PrepareImageSeg',
                downsample=1,
                is_train=True,
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(512, 1408),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True,
                img_seg_dir='data/nuscenes/imgseg/samples'),
            dict(type='LoadOccGTFromFile'),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='FuseAdjacentSweeps',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(type='PointsLidar2Ego'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
            dict(
                type='LoadAnnotationsAll',
                bda_aug_conf=dict(
                    rot_lim=(-0.0, 0.0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                is_train=True),
            dict(
                type='PointToMultiViewDepth',
                downsample=1,
                grid_config=dict(
                    x=[-40, 40, 0.4],
                    y=[-40, 40, 0.4],
                    z=[-1, 5.4, 0.4],
                    depth=[1.0, 45.0, 0.5])),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=[
                    'img_inputs', 'points', 'sparse_depth', 'segs',
                    'voxel_semantics', 'mask_camera'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True,
        use_mask=True,
        stereo=False,
        filter_empty_gt=False,
        img_info_prototype='fusionocc',
        multi_adj_frame_id_cfg=(1, 2, 1),
        multi_adj_frame_id_cfg_lidar=(1, 8, 1)),
    val=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/fusionocc-nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='PrepareImageSeg',
                restore_upsample=8,
                downsample=1,
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(512, 1408),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True,
                img_seg_dir='data/nuscenes/imgseg/samples'),
            dict(type='LoadOccGTFromFile'),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='FuseAdjacentSweeps',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(type='PointsLidar2Ego'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
            dict(
                type='LoadAnnotationsAll',
                bda_aug_conf=dict(
                    rot_lim=(-0.0, 0.0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                is_train=False),
            dict(
                type='PointToMultiViewDepth',
                downsample=1,
                grid_config=dict(
                    x=[-40, 40, 0.4],
                    y=[-40, 40, 0.4],
                    z=[-1, 5.4, 0.4],
                    depth=[1.0, 45.0, 0.5])),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='Collect3D',
                        keys=['img_inputs', 'points', 'sparse_depth'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR',
        use_mask=True,
        stereo=False,
        filter_empty_gt=False,
        img_info_prototype='fusionocc',
        multi_adj_frame_id_cfg=(1, 2, 1),
        multi_adj_frame_id_cfg_lidar=(1, 8, 1)),
    test=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/fusionocc-nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='PrepareImageSeg',
                restore_upsample=8,
                downsample=1,
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(512, 1408),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True,
                img_seg_dir='data/nuscenes/imgseg/samples'),
            dict(type='LoadOccGTFromFile'),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='FuseAdjacentSweeps',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(type='PointsLidar2Ego'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
            dict(
                type='LoadAnnotationsAll',
                bda_aug_conf=dict(
                    rot_lim=(-0.0, 0.0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                is_train=False),
            dict(
                type='PointToMultiViewDepth',
                downsample=1,
                grid_config=dict(
                    x=[-40, 40, 0.4],
                    y=[-40, 40, 0.4],
                    z=[-1, 5.4, 0.4],
                    depth=[1.0, 45.0, 0.5])),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='Collect3D',
                        keys=['img_inputs', 'points', 'sparse_depth'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR',
        use_mask=True,
        stereo=False,
        filter_empty_gt=False,
        img_info_prototype='fusionocc',
        multi_adj_frame_id_cfg=(1, 2, 1),
        multi_adj_frame_id_cfg_lidar=(1, 8, 1)))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='PrepareImageSeg',
            restore_upsample=8,
            downsample=1,
            data_config=dict(
                cams=[
                    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                ],
                Ncams=6,
                input_size=(512, 1408),
                src_size=(900, 1600),
                resize=(-0.06, 0.11),
                rot=(-5.4, 5.4),
                flip=True,
                crop_h=(0.0, 0.0),
                resize_test=0.0),
            sequential=True,
            img_seg_dir='data/nuscenes/imgseg/samples'),
        dict(type='LoadOccGTFromFile'),
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='FuseAdjacentSweeps',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(type='PointsLidar2Ego'),
        dict(
            type='PointsRangeFilter',
            point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
        dict(
            type='LoadAnnotationsAll',
            bda_aug_conf=dict(
                rot_lim=(-0.0, 0.0),
                scale_lim=(1.0, 1.0),
                flip_dx_ratio=0.5,
                flip_dy_ratio=0.5),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            is_train=False),
        dict(
            type='PointToMultiViewDepth',
            downsample=1,
            grid_config=dict(
                x=[-40, 40, 0.4],
                y=[-40, 40, 0.4],
                z=[-1, 5.4, 0.4],
                depth=[1.0, 45.0, 0.5])),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='Collect3D',
                    keys=['img_inputs', 'points', 'sparse_depth'])
            ])
    ])
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/fusion_occ'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
data_config = dict(
    cams=[
        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',
        'CAM_BACK', 'CAM_BACK_RIGHT'
    ],
    Ncams=6,
    input_size=(512, 1408),
    src_size=(900, 1600),
    resize=(-0.06, 0.11),
    rot=(-5.4, 5.4),
    flip=True,
    crop_h=(0.0, 0.0),
    resize_test=0.0)
grid_config = dict(
    x=[-40, 40, 0.4],
    y=[-40, 40, 0.4],
    z=[-1, 5.4, 0.4],
    depth=[1.0, 45.0, 0.5])
use_mask = True
voxel_size = [0.05, 0.05, 0.05]
img_backbone_out_channel = 256
feature_channel = 32
lidar_out_channel = 32
img_channels = 32
numC_Trans = 64
num_classes = 18
multi_adj_frame_id_cfg = (1, 2, 1)
multi_adj_frame_id_cfg_lidar = (1, 8, 1)
model = dict(
    type='FusionOCC',
    lidar_in_channel=5,
    point_cloud_range=[-40, -40, -1, 40, 40, 5.4],
    voxel_size=[0.05, 0.05, 0.05],
    lidar_out_channel=32,
    align_after_view_transformation=True,
    num_adj=1,
    fuse_loss_weight=0.1,
    img_backbone=dict(
        type='SwinTransformer',
        pretrain_img_size=224,
        patch_size=4,
        window_size=12,
        mlp_ratio=4,
        embed_dims=128,
        depths=[2, 2, 18, 2],
        num_heads=[4, 8, 16, 32],
        strides=(4, 2, 2, 2),
        out_indices=(2, 3),
        qkv_bias=True,
        qk_scale=None,
        patch_norm=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1,
        use_abs_pos_embed=False,
        return_stereo_feat=True,
        act_cfg=dict(type='GELU'),
        norm_cfg=dict(type='LN', requires_grad=True),
        pretrain_style='official',
        output_missing_index_as_none=False),
    img_neck=dict(
        type='FPN_LSS',
        in_channels=1536,
        out_channels=256,
        extra_upsample=None,
        input_feature_index=(0, 1),
        scale_factor=2),
    img_view_transformer=dict(
        type='CrossModalLSS',
        feature_channels=32,
        seg_num_classes=18,
        grid_config=dict(
            x=[-40, 40, 0.4],
            y=[-40, 40, 0.4],
            z=[-1, 5.4, 0.4],
            depth=[1.0, 45.0, 0.5]),
        input_size=(512, 1408),
        in_channels=256,
        mid_channels=128,
        depth_channels=88,
        is_train=True,
        out_channels=32,
        sid=False,
        collapse_z=False,
        depthnet_cfg=dict(aspp_mid_channels=96),
        downsample=16),
    pre_process=dict(
        type='CustomResNet3D',
        numC_input=32,
        with_cp=False,
        num_layer=[1],
        num_channels=[32],
        stride=[1],
        backbone_output_ids=[0]),
    occ_encoder_backbone=dict(
        type='CustomResNet3D',
        numC_input=96,
        num_layer=[1, 2, 3],
        with_cp=False,
        num_channels=[64, 128, 256],
        stride=[1, 2, 2],
        backbone_output_ids=[0, 1, 2]),
    occ_encoder_neck=dict(type='LSSFPN3D', in_channels=448, out_channels=64),
    out_dim=64,
    loss_occ=dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
    use_mask=True)
img_seg_dir = 'data/nuscenes/imgseg/samples'
bda_aug_conf = dict(
    rot_lim=(-0.0, 0.0),
    scale_lim=(1.0, 1.0),
    flip_dx_ratio=0.5,
    flip_dy_ratio=0.5)
share_data_config = dict(
    use_mask=True,
    type='NuScenesDatasetOccpancy',
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    modality=dict(
        use_lidar=False,
        use_camera=True,
        use_radar=False,
        use_map=False,
        use_external=False),
    stereo=False,
    filter_empty_gt=False,
    img_info_prototype='fusionocc',
    multi_adj_frame_id_cfg=(1, 2, 1),
    multi_adj_frame_id_cfg_lidar=(1, 8, 1))
test_data_config = dict(
    pipeline=[
        dict(
            type='PrepareImageSeg',
            restore_upsample=8,
            downsample=1,
            data_config=dict(
                cams=[
                    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                ],
                Ncams=6,
                input_size=(512, 1408),
                src_size=(900, 1600),
                resize=(-0.06, 0.11),
                rot=(-5.4, 5.4),
                flip=True,
                crop_h=(0.0, 0.0),
                resize_test=0.0),
            sequential=True,
            img_seg_dir='data/nuscenes/imgseg/samples'),
        dict(type='LoadOccGTFromFile'),
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='FuseAdjacentSweeps',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(type='PointsLidar2Ego'),
        dict(
            type='PointsRangeFilter',
            point_cloud_range=[-40, -40, -1, 40, 40, 5.4]),
        dict(
            type='LoadAnnotationsAll',
            bda_aug_conf=dict(
                rot_lim=(-0.0, 0.0),
                scale_lim=(1.0, 1.0),
                flip_dx_ratio=0.5,
                flip_dy_ratio=0.5),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            is_train=False),
        dict(
            type='PointToMultiViewDepth',
            downsample=1,
            grid_config=dict(
                x=[-40, 40, 0.4],
                y=[-40, 40, 0.4],
                z=[-1, 5.4, 0.4],
                depth=[1.0, 45.0, 0.5])),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='Collect3D',
                    keys=['img_inputs', 'points', 'sparse_depth'])
            ])
    ],
    ann_file='data/nuscenes/fusionocc-nuscenes_infos_val.pkl',
    use_mask=True,
    type='NuScenesDatasetOccpancy',
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    modality=dict(
        use_lidar=False,
        use_camera=True,
        use_radar=False,
        use_map=False,
        use_external=False),
    stereo=False,
    filter_empty_gt=False,
    img_info_prototype='fusionocc',
    multi_adj_frame_id_cfg=(1, 2, 1),
    multi_adj_frame_id_cfg_lidar=(1, 8, 1))
key = 'test'
optimizer = dict(type='AdamW', lr=5e-05, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=5, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
runner = dict(type='EpochBasedRunner', max_epochs=24)
custom_hooks = [
    dict(type='MEGVIIEMAHook', init_updates=10560, priority='NORMAL'),
    dict(type='SyncbnControlHook', syncbn_start_epoch=0)
]
gpu_ids = [0]

2025-07-19 11:50:15,266 - mmdet - INFO - Set random seed to 0, deterministic: False
Name of parameter - Initialization information

img_backbone.patch_embed.projection.weight - torch.Size([128, 3, 4, 4]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.patch_embed.projection.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.patch_embed.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.patch_embed.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([512, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([512, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.downsample.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.downsample.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.0.downsample.reduction.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.downsample.norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.downsample.norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.1.downsample.reduction.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.downsample.norm.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.downsample.norm.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.norm3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_backbone.norm3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.0.weight - torch.Size([256, 1536, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_neck.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.0.weight - torch.Size([128, 88, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_encoder.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.img_reduce_conv.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.img_reduce_conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.img_reduce_conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.img_reduce_conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.channel_mlp_c.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.channel_mlp_c.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.channel_mlp_d.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.channel_mlp_d.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_c.0.weight - torch.Size([64, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_c.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_c.2.weight - torch.Size([1, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_c.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_d.0.weight - torch.Size([64, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_d.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_d.2.weight - torch.Size([1, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.spatial_d.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.fuse_conv.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.fuse_conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.fuse_conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.cross_model_fusion.fuse_conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.further_fuse.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_depth.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_depth.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_depth.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_depth.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_seg.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_seg.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_seg.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_seg.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_context.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_context.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_context.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.reduce_conv_context.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_mlp.fc1.weight - torch.Size([128, 27]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_mlp.fc1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_mlp.fc2.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.bn.weight - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.bn.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_se.conv_reduce.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_se.conv_reduce.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_se.conv_expand.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_se.conv_expand.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp1.atrous_conv.weight - torch.Size([96, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp1.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp1.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp2.atrous_conv.weight - torch.Size([96, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp2.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp2.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp3.atrous_conv.weight - torch.Size([96, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp3.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp3.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp4.atrous_conv.weight - torch.Size([96, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp4.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.aspp4.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.global_avg_pool.1.weight - torch.Size([96, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.global_avg_pool.2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.global_avg_pool.2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.conv1.weight - torch.Size([128, 480, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.3.weight - torch.Size([88, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.depth_conv.3.bias - torch.Size([88]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_mlp.fc1.weight - torch.Size([128, 27]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_mlp.fc1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_mlp.fc2.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_se.conv_reduce.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_se.conv_reduce.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_se.conv_expand.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_se.conv_expand.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_conv.weight - torch.Size([16, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.context_conv.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_mlp.fc1.weight - torch.Size([128, 27]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_mlp.fc1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_mlp.fc2.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_se.conv_reduce.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_se.conv_reduce.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_se.conv_expand.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_se.conv_expand.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.0.weight - torch.Size([16, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.0.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.conv1.weight - torch.Size([16, 16, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.conv2.weight - torch.Size([16, 16, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_conv.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_out.weight - torch.Size([18, 16, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_view_transformer.depth_seg_net.seg_out.bias - torch.Size([18]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv1.conv.weight - torch.Size([64, 96, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv1.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv1.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv2.conv.weight - torch.Size([64, 64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv2.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.conv2.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.downsample.conv.weight - torch.Size([64, 96, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.downsample.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.0.0.downsample.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv1.conv.weight - torch.Size([128, 64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv1.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv1.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv2.conv.weight - torch.Size([128, 128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv2.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.conv2.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.downsample.conv.weight - torch.Size([128, 64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.downsample.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.0.downsample.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv1.conv.weight - torch.Size([128, 128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv1.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv1.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv2.conv.weight - torch.Size([128, 128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv2.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.1.1.conv2.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv1.conv.weight - torch.Size([256, 128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv2.conv.weight - torch.Size([256, 256, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.conv2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.downsample.conv.weight - torch.Size([256, 128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.downsample.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.0.downsample.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv1.conv.weight - torch.Size([256, 256, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv2.conv.weight - torch.Size([256, 256, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.1.conv2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv1.conv.weight - torch.Size([256, 256, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv2.conv.weight - torch.Size([256, 256, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_backbone.layers.2.2.conv2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_neck.conv.conv.weight - torch.Size([64, 448, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_neck.conv.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

img_bev_encoder_neck.conv.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv1.conv.weight - torch.Size([32, 32, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv1.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv1.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv2.conv.weight - torch.Size([32, 32, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv2.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.conv2.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.downsample.conv.weight - torch.Size([32, 32, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.downsample.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

pre_process_net.layers.0.0.downsample.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.conv_input.0.weight - torch.Size([1, 1, 1, 5, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.0.0.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.1.0.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.1.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.1.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.2.0.weight - torch.Size([3, 3, 3, 16, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.2.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer1.2.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.0.0.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.1.0.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.2.0.weight - torch.Size([3, 3, 3, 32, 48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.2.1.weight - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer2.2.1.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.0.0.weight - torch.Size([3, 3, 3, 48, 48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.0.1.weight - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.0.1.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.1.0.weight - torch.Size([3, 3, 3, 48, 48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.1.1.weight - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.1.1.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.2.0.weight - torch.Size([3, 3, 3, 48, 64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer3.2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.0.0.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.1.0.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.encoder_layers.encoder_layer4.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

lidar_encoder.conv_out.0.weight - torch.Size([1, 1, 1, 64, 32]): 
The value is the same before and after calling `init_weights` of FusionOCC  

final_conv.conv.weight - torch.Size([64, 64, 3, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

final_conv.conv.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

predicter.0.weight - torch.Size([128, 64]): 
The value is the same before and after calling `init_weights` of FusionOCC  

predicter.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

predicter.2.weight - torch.Size([18, 128]): 
The value is the same before and after calling `init_weights` of FusionOCC  

predicter.2.bias - torch.Size([18]): 
The value is the same before and after calling `init_weights` of FusionOCC  
2025-07-19 11:50:15,911 - mmdet - INFO - Model:
FusionOCC(
  (img_backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=128, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=512, out_features=128, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=128, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=512, out_features=128, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=512, out_features=256, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=1024, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1024, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=1024, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1024, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): SwinBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=512, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
      )
    )
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (img_neck): FPN_LSS(
    (up): Upsample(scale_factor=2.0, mode=bilinear)
    (conv): Sequential(
      (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (img_view_transformer): CrossModalLSS(
    (depth_net): None
    (depth_encoder): Sequential(
      (0): Conv2d(88, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (img_reduce_conv): Sequential(
      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (cross_model_fusion): CrossModalFusion(
      (channel_mlp_c): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): Sigmoid()
      )
      (channel_mlp_d): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): Sigmoid()
      )
      (spatial_c): Sequential(
        (0): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
        (3): ReLU(inplace=True)
      )
      (spatial_d): Sequential(
        (0): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
        (3): ReLU(inplace=True)
      )
      (fuse_conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (further_fuse): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (depth_seg_net): DepthSegNet(
      (reduce_conv_depth): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (reduce_conv_seg): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (reduce_conv_context): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (depth_mlp): Mlp(
        (fc1): Linear(in_features=27, out_features=128, bias=True)
        (act): ReLU()
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=128, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (bn): BatchNorm1d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (depth_se): SELayer(
        (conv_reduce): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU()
        (conv_expand): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (depth_conv): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): ASPP(
          (aspp1): _ASPPModule(
            (atrous_conv): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
          )
          (aspp2): _ASPPModule(
            (atrous_conv): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
          )
          (aspp3): _ASPPModule(
            (atrous_conv): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
          )
          (aspp4): _ASPPModule(
            (atrous_conv): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
          )
          (global_avg_pool): Sequential(
            (0): AdaptiveAvgPool2d(output_size=(1, 1))
            (1): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
          )
          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.5, inplace=False)
        )
        (3): Conv2d(128, 88, kernel_size=(1, 1), stride=(1, 1))
      )
      (context_mlp): Mlp(
        (fc1): Linear(in_features=27, out_features=128, bias=True)
        (act): ReLU()
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=128, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (context_se): SELayer(
        (conv_reduce): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU()
        (conv_expand): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (context_conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (seg_mlp): Mlp(
        (fc1): Linear(in_features=27, out_features=128, bias=True)
        (act): ReLU()
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=128, out_features=128, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (seg_se): SELayer(
        (conv_reduce): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU()
        (conv_expand): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (seg_conv): Sequential(
        (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BasicBlock(
          (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (seg_out): Conv2d(16, 18, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (img_bev_encoder_backbone): CustomResNet3D(
    (layers): Sequential(
      (0): Sequential(
        (0): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(96, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (downsample): ConvModule(
            (conv): Conv3d(96, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
      )
      (1): Sequential(
        (0): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (downsample): ConvModule(
            (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
        (1): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
      )
      (2): Sequential(
        (0): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (downsample): ConvModule(
            (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
        (1): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
        (2): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
      )
    )
  )
  (img_bev_encoder_neck): LSSFPN3D(
    (up1): Upsample(scale_factor=2.0, mode=trilinear)
    (up2): Upsample(scale_factor=4.0, mode=trilinear)
    (conv): ConvModule(
      (conv): Conv3d(448, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
  )
  (pre_process_net): CustomResNet3D(
    (layers): Sequential(
      (0): Sequential(
        (0): BasicBlock3D(
          (conv1): ConvModule(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (downsample): ConvModule(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
            (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(inplace=True)
        )
      )
    )
  )
  (lidar_encoder): CustomSparseEncoder(
    (conv_input): SparseSequential(
      (0): SubMConv3d()
    )
    (encoder_layers): SparseSequential(
      (encoder_layer1): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer2): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer3): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer4): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SubMConv3d()
    )
  )
  (final_conv): ConvModule(
    (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (activate): ReLU(inplace=True)
  )
  (predicter): Sequential(
    (0): Linear(in_features=64, out_features=128, bias=True)
    (1): Softplus(beta=1, threshold=20)
    (2): Linear(in_features=128, out_features=18, bias=True)
  )
  (loss_occ): CrossEntropyLoss(avg_non_ignore=False)
)
2025-07-19 11:50:28,945 - mmdet - INFO - Start running, host: root@dbd8ae03e22c, work_dir: /workspaces/FusionOcc/work_dirs/fusion_occ
2025-07-19 11:50:28,946 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) SyncbnControlHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-07-19 11:50:28,946 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs
2025-07-19 11:50:28,946 - mmdet - INFO - Checkpoints will be saved to /workspaces/FusionOcc/work_dirs/fusion_occ by HardDiskBackend.
